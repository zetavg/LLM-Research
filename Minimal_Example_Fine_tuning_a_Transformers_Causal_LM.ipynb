{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zetavg/LLM-Research/blob/8130726/Minimal_Example_Fine_tuning_a_Transformers_Causal_LM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da1191fb-8f5a-41c4-9e3b-82a74b018c47",
      "metadata": {
        "id": "da1191fb-8f5a-41c4-9e3b-82a74b018c47"
      },
      "source": [
        "# Minimal Example: Fine-tuning a Transformers Causal LM\n",
        "\n",
        "A minimal example of fine-tuning a causal language model (LLaMA, GPT-J, etc.) with 🤗 Transformers's Trainer.\n",
        "\n",
        "Run the code cells one by one to and see their outputs.\n",
        "\n",
        "(For a even more minimal version of this, check out the [Very Minimal Example: Fine-tuning a Transformers Causal LM](https://github.com/zetavg/LLM-Research/blob/39511ae/Very_Minimal_Example_Fine_tuning_a_Transformers_Causal_LM.ipynb).)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a948c5cb-408c-46da-ba69-b5bc5e4ef239",
      "metadata": {
        "id": "a948c5cb-408c-46da-ba69-b5bc5e4ef239"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "(~30sec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "156a8136-6795-4209-9388-313070e7ba89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "156a8136-6795-4209-9388-313070e7ba89",
        "outputId": "8e2bd7c4-5a9c-46f1-d0bb-06cdfe72ce88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: transformers==4.28.1 in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: datasets==2.12.0 in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (3.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.1) (23.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (3.8.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (0.70.14)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (1.5.3)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (0.3.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.12.0) (2023.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (23.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.12.0) (1.9.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.1) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.12.0) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.12.0) (2.8.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.12.0) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers==4.28.1 datasets==2.12.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78cac29a-ad64-4bf3-9d3a-10e10640f713",
      "metadata": {
        "id": "78cac29a-ad64-4bf3-9d3a-10e10640f713"
      },
      "source": [
        "## Get the Device Type\n",
        "\n",
        "So that subsequent code can place the model and stuff on the correct device. (~10sec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7e981c16-63fc-499e-9e85-a66be3d881c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e981c16-63fc-499e-9e85-a66be3d881c7",
        "outputId": "f1a89089-f9eb-4fda-b323-5d10e3b5a703"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3336dfa-18aa-4f7f-8b8b-ddba13c9b634",
      "metadata": {
        "id": "c3336dfa-18aa-4f7f-8b8b-ddba13c9b634"
      },
      "source": [
        "## Load the Model and Tokenizer\n",
        "\n",
        "(~10 sec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "79eabcd9-a25d-47bf-91bd-98aa81099e57",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79eabcd9-a25d-47bf-91bd-98aa81099e57",
        "outputId": "b7b4751a-c934-4e66-e77c-865db183b46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "Model loaded.\n",
            "Loading tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer has no pad_token set, setting it to eos_token (<|endoftext|>).\n",
            "Tokenizer loaded.\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = None\n",
        "model = None\n",
        "\n",
        "# Here we use a relatively small model. Training larger models on Colab will get\n",
        "# to CUDA Out-Of-Memory really quick.\n",
        "tokenizer_name = \"EleutherAI/pythia-70m\"\n",
        "model_name = \"EleutherAI/pythia-70m\"\n",
        "\n",
        "def get_tokenizer():\n",
        "    clear_cache()\n",
        "    print('Loading tokenizer...')\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "\n",
        "    # if no pad token, set it to eos\n",
        "    if tokenizer.pad_token is None:\n",
        "        print(\n",
        "            f\"Tokenizer has no pad_token set, setting it to eos_token ({tokenizer.eos_token}).\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print('Tokenizer loaded.')\n",
        "    return tokenizer\n",
        "\n",
        "def get_model():\n",
        "    clear_cache()\n",
        "    print('Loading model...')\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model = model.to(device)  # move to device (GPU if available)\n",
        "    print('Model loaded.')\n",
        "    return model\n",
        "\n",
        "\n",
        "def clear_cache():\n",
        "    # To avoid eating up GPU RAM.\n",
        "    # Not sure if this works. At least we try.\n",
        "    gc.collect()\n",
        "    with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "model = get_model()\n",
        "tokenizer = get_tokenizer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed95575d-f06e-455f-9391-ab30b9565103",
      "metadata": {
        "id": "ed95575d-f06e-455f-9391-ab30b9565103"
      },
      "source": [
        "## Test the Model Before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "effa8cca-fca9-41b7-a094-017860405f40",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "effa8cca-fca9-41b7-a094-017860405f40",
        "outputId": "6f70ee5b-7b58-4705-b568-a11fa8f7e69c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training: False (should be False)\n",
            "\n",
            "input_ids: tensor([[1552,  310]], device='cuda:0')\n",
            "output_ids: tensor([1552,  310,  247, 1270, 1650,  273,  849,  436,  310, 1469,  281,  320,\n",
            "         247, 1270, 1650,  273,  849,  436,  310, 1469,  281,  320,  247, 1270,\n",
            "        1650,  273,  849,  436,  310, 1469,  281,  320], device='cuda:0')\n",
            "\n",
            "prompt: This is\n",
            "generated_text: This is a great example of how this is going to be a great example of how this is going to be a great example of how this is going to be\n"
          ]
        }
      ],
      "source": [
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "print(\"Model training:\", model.training, \"(should be False)\")\n",
        "\n",
        "# Tokenize the prompt into tensor of token IDs\n",
        "prompt = \"This is\"\n",
        "input_ids = tokenizer(\n",
        "    prompt,\n",
        "    return_tensors=\"pt\"  # Let it return PyTorch (`pt`) tensors\n",
        ").input_ids\n",
        "# Send values to device (GPU)\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "# Let the model generate the completion\n",
        "output_sequences = model.generate(input_ids, max_length=32)\n",
        "output_ids = output_sequences[0]\n",
        "generated_text = tokenizer.decode(output_ids)\n",
        "\n",
        "# Print the results\n",
        "print()\n",
        "print(\"input_ids:\", input_ids)\n",
        "print(\"output_ids:\", output_ids)\n",
        "print()\n",
        "print(\"prompt:\", prompt)\n",
        "print(\"generated_text:\", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74cb9ff6-0a14-45f7-a5e9-905c1e2968bd",
      "metadata": {
        "id": "74cb9ff6-0a14-45f7-a5e9-905c1e2968bd"
      },
      "source": [
        "## The Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "93de03d8-6486-4e44-a854-b2fdc2507b70",
      "metadata": {
        "id": "93de03d8-6486-4e44-a854-b2fdc2507b70"
      },
      "outputs": [],
      "source": [
        "# If the model behavies wierd durning or after the training, \n",
        "# uncomment the following lines to avoid training from a model instance that\n",
        "# has been already used to generate text. This might fix the issue.\n",
        "\n",
        "# model = get_model()\n",
        "# tokenizer = get_tokenizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Train Data\n",
        "\n",
        "While we will normally load the dataset from elsewhere using the [`load_dataset()` function](https://huggingface.co/docs/datasets/loading).\n",
        "\n",
        "But for simplicity and transparency here we'll just define our train data with a small list."
      ],
      "metadata": {
        "id": "rX6EE86JD0Ii"
      },
      "id": "rX6EE86JD0Ii"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "items = [\n",
        "    {'text': \"This is a great language model. Meow meow meow, meow meow. Oh, I'm not a cat. Meow.\"},\n",
        "    {'text': \"The quick brown fox jumps over the lazy dog.\"},\n",
        "    {'text': \"A book can't decide everything. For greater hope, I just know lots more nuances of possibilities.\"},\n",
        "]\n",
        "\n",
        "ds = Dataset.from_list(items)\n",
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imrDqs-1D2us",
        "outputId": "cea637b3-1125-4d37-ab32-670d9eb64e9a"
      },
      "id": "imrDqs-1D2us",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 3\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before feeding our data into the trainer, we'll need to convert each of them into tokenized `input_ids` and `labels`."
      ],
      "metadata": {
        "id": "1g8BVF_ffdsF"
      },
      "id": "1g8BVF_ffdsF"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(data_point):\n",
        "    batch_encoding = tokenizer(\n",
        "        # See: https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#tokenizer\n",
        "        data_point['text'],\n",
        "        max_length=32,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        # return_tensors=\"pt\"  # This is handled by the trainer.\n",
        "    )\n",
        "    batch_encoding[\"labels\"] = batch_encoding[\"input_ids\"].copy()\n",
        "    # This is handled by the trainer.\n",
        "    # batch_encoding = {k: v.to(device) for k, v in batch_encoding.items()}\n",
        "    return batch_encoding\n",
        "\n",
        "train_data = ds.map(tokenize_data)\n",
        "\n",
        "import json\n",
        "print(\"Sample item:\")\n",
        "print(json.dumps(train_data[1]).replace('{', '{\\n  ').replace('\",', '\",\\n ').replace('],', '],\\n ').replace('}', '\\n}'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "223f3a0aac2c4967b3c6e2987d1bf870",
            "a29eff3ec1424ca8887a0ef94a3c5f84",
            "ae47417f22a44313a8a3e93c651d6a8a",
            "23704bb22bce437e8daa47f4a9bf72be",
            "b612c0f9a920493b8bdc646df1ae2f80",
            "d20c10ff506945d08bd8334d4fe3c1a5",
            "cadd1d5ef7bd4998b9e00c64410141a2",
            "bcd4ed79b9184062b51ce6e4be213982",
            "3b9bfda7faf645e6a7b00ef99dc875eb",
            "59a183ccacea40d78b2efbeccc156dbb",
            "0f8d6072c6564226850d47d72aa99aef"
          ]
        },
        "id": "L-HsceIRfcFe",
        "outputId": "f1c2d380-f2f9-4cd6-85c8-20d7d9914560"
      },
      "id": "L-HsceIRfcFe",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "223f3a0aac2c4967b3c6e2987d1bf870"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample item:\n",
            "{\n",
            "  \"text\": \"The quick brown fox jumps over the lazy dog.\",\n",
            "  \"input_ids\": [510, 3158, 8516, 30013, 27287, 689, 253, 22658, 4370, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "  \"labels\": [510, 3158, 8516, 30013, 27287, 689, 253, 22658, 4370, 15, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Training Arguments\n",
        "\n",
        "See [the docs](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) for more info about the arguments."
      ],
      "metadata": {
        "id": "mBFV-Z5EHX1H"
      },
      "id": "mBFV-Z5EHX1H"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# See: https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./training_output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=7,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"adamw_torch\",\n",
        "    learning_rate=5e-5,\n",
        "    logging_steps=5\n",
        ")"
      ],
      "metadata": {
        "id": "D4VoXf31Ham1"
      },
      "id": "D4VoXf31Ham1",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the Trainer"
      ],
      "metadata": {
        "id": "DQSAp_9yIkpp"
      },
      "id": "DQSAp_9yIkpp"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer"
      ],
      "metadata": {
        "id": "mP0feYA5c_Um"
      },
      "id": "mP0feYA5c_Um",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Some advanced stuff - just for printing more info while training\n",
        "\n",
        "⬇️ You can just press the play button to execute this and check back what's happening here later."
      ],
      "metadata": {
        "id": "IAGhwFk9cURy"
      },
      "id": "IAGhwFk9cURy"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# @markdown Here the `transformers.Trainer` class is subclass-ed and have the `training_step` and `compute_loss` functions overridden to print additional information, showing what's going on under the hood.\n",
        "class CustomTrainer(Trainer):\n",
        "    def training_step(self, model, inputs):\n",
        "        tensor = super().training_step(model, inputs)\n",
        "\n",
        "        # Do not print info on the first step to avoid \n",
        "        # messing up with the tqdm progress bar.\n",
        "        if hasattr(self, \"not_first_step\"):\n",
        "            time.sleep(3)  # Just for visual effects - so that we can see each step one by one.\n",
        "            print(\"Step completed.\")\n",
        "            print()\n",
        "\n",
        "        self.not_first_step = True\n",
        "        return tensor\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        loss, outputs = super().compute_loss(\n",
        "            model, inputs,\n",
        "            # force the original `training_step` to return outputs \n",
        "            # so we can inspect it\n",
        "            return_outputs=True\n",
        "        )\n",
        "\n",
        "        # Do not print info on the first step to avoid \n",
        "        # messing up with the tqdm progress bar.\n",
        "        if hasattr(self, \"not_first_step\"):\n",
        "              # Preview what the model have generated\n",
        "              logits = outputs.logits\n",
        "              # Get the token IDs with the highest probabilities\n",
        "              token_ids = logits.argmax(dim=-1).squeeze().tolist()\n",
        "              generated_text = tokenizer.decode(token_ids)  # Decode the token ids\n",
        "              print(\"Target:\", tokenizer.decode(inputs['labels'][0][1:]))\n",
        "              print(\"Actual:\", generated_text)\n",
        "\n",
        "              # Print information about the train step\n",
        "              print(\"Loss:\", loss)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# @markdown Normally, something like the code in this cell will not appear in actual training script, except you want to override the trainer for custom behavior.\n",
        "Trainer = CustomTrainer"
      ],
      "metadata": {
        "id": "Q9FbQJBiY5W8"
      },
      "id": "Q9FbQJBiY5W8",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the Trainer\n",
        "\n",
        "Create the trainer with the defined `training_args`. See [the docs](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) for more info about the arguments."
      ],
      "metadata": {
        "id": "PSDDuEQyaO-p"
      },
      "id": "PSDDuEQyaO-p"
    },
    {
      "cell_type": "code",
      "source": [
        "# See: https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    args=training_args\n",
        ")"
      ],
      "metadata": {
        "id": "6c5bfhcVInBz"
      },
      "id": "6c5bfhcVInBz",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start the Training!\n",
        "\n",
        "Since we have overwritten the `Trainer` class in the \"Some advanced stuff\" block above, some details that will normally not shown will be printed during training. \n",
        "\n",
        "Also, a delay between steps is added for the convenience of checking each step one by one.\n",
        "\n",
        "You can observe the dropping loss and the actual `output` getting closer to the target `label` on each step."
      ],
      "metadata": {
        "id": "1wGegrOBcpz5"
      },
      "id": "1wGegrOBcpz5"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "model.save_pretrained(\"./trained_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K9At8k94NOFv",
        "outputId": "f0e9bb24-5abb-42f0-ebb2-b9c5ea6f3504"
      },
      "id": "K9At8k94NOFv",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 01:01, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.158600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.183200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.332300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.101200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target:  quick brown fox jumps over the lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual: <|endoftext|> and and can over the moon,.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(1.7123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  is a great language model. Meow meow meow, meow meow. Oh, I'm not a cat. Meow.<|endoftext|><|endoftext|>\n",
            "Actual: <|endoftext|> a great book..<|endoftext|><|endoftext|>. with.ow I Iow,ow,<|endoftext|>, I know just a big.<|endoftext|>ow.<|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(4.3982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  quick brown fox jumps over the lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual: <|endoftext|>. fox jumps over the lazy doggett<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.7036, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  is a great language model. Meow meow meow, meow meow. Oh, I'm not a cat. Meow.<|endoftext|><|endoftext|>\n",
            "Actual: <|endoftext|> a great book..<|endoftext|>ow meow meow me meow meow me me me meow a a dog.<|endoftext|>ow me Me<|endoftext|><|endoftext|>\n",
            "Loss: tensor(2.7378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  book can't decide everything. For greater hope, I just know lots more nuances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual: <|endoftext|> can't decide..<|endoftext|> more hope. hope hope know more more possibilitiesances. possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(1.9253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  is a great language model. Meow meow meow, meow meow. Oh, I'm not a cat. Meow.<|endoftext|><|endoftext|>\n",
            "Actual:  is a great book language model<|endoftext|>ow meow meow. meow.ow. Me, I'm a a cat. Meow. Me<|endoftext|><|endoftext|>\n",
            "Loss: tensor(1.4610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  quick brown fox jumps over the lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual: The quick brown fox fox fox lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(1.2073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  book can't decide everything. For greater hope, I just know lots more nuances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual:  is book't decide everything.<|endoftext|> me hope, I just know everything more possibilitiesances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.8615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  book can't decide everything. For greater hope, I just know lots more nuances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual:  book can't decide everything.<|endoftext|> a hope, I just know lots more nuances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.4608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  is a great language model. Meow meow meow, meow meow. Oh, I'm not a cat. Meow.<|endoftext|><|endoftext|>\n",
            "Actual:  is a great language model. Iow meow meow me meow.ow. Me, I'm not a cat. Iow. Oh<|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.6004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  quick brown fox jumps over the lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual:  book quick brown jumps over. lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.2791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  quick brown fox jumps over the lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual:  book brown fox jumps over the lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  is a great language model. Meow meow meow, meow meow. Oh, I'm not a cat. Meow.<|endoftext|><|endoftext|>\n",
            "Actual:  is a great language model. Forow meow meow me meow meow me Me, I'm not a cat. Meow me Me<|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.5442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  book can't decide everything. For greater hope, I just know lots more nuances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual:  book can't decide everything. For greater hope, I just know lots more nuances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  quick brown fox jumps over the lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual:  quick brown fox jumps over the lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.0324, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  is a great language model. Meow meow meow, meow meow. Oh, I'm not a cat. Meow.<|endoftext|><|endoftext|>\n",
            "Actual:  is a great language model. Forow meow meow me meow meow. Me, I'm not a cat. Meow.<|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.3229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  book can't decide everything. For greater hope, I just know lots more nuances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual:  book can't decide everything. For greater hope, I just know lots more nuances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.0752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  book can't decide everything. For greater hope, I just know lots more nuances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual:  book can't decide everything. For greater hope, I just know lots more nuances of possibilities.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.0616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  quick brown fox jumps over the lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Actual:  quick brown fox jumps over the lazy dog.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n",
            "Target:  is a great language model. Meow meow meow, meow meow. Oh, I'm not a cat. Meow.<|endoftext|><|endoftext|>\n",
            "Actual:  is a great language model. Forow meow meow. meow.ow.<|endoftext|>, I'm not a cat. Meow.<|endoftext|><|endoftext|><|endoftext|>\n",
            "Loss: tensor(0.3183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Step completed.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the Trained Model\n",
        "\n",
        "Now the model has been trained, we can set it to evaluation mode and use the `generate` function to test it."
      ],
      "metadata": {
        "id": "CiXEcSc-y8O7"
      },
      "id": "CiXEcSc-y8O7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to evaluation mode\n",
        "model.eval()\n",
        "print(\"Model training:\", model.training, \"(should be False)\")\n",
        "\n",
        "prompt = \"This is a\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "output_sequences = model.generate(input_ids, max_length=32)\n",
        "output_ids = output_sequences[0]\n",
        "generated_text = tokenizer.decode(output_ids)\n",
        "\n",
        "print()\n",
        "print(\"input_ids:\", input_ids)\n",
        "print(\"output_ids:\", output_ids)\n",
        "print()\n",
        "print(\"prompt:\", prompt)\n",
        "print(\"generated_text:\", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfTUCe6Cy9Zq",
        "outputId": "d23561a6-6230-4d61-f16c-3495574bbdf6"
      },
      "id": "FfTUCe6Cy9Zq",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training: False (should be False)\n",
            "\n",
            "input_ids: tensor([[1552,  310,  247]], device='cuda:0')\n",
            "output_ids: tensor([1552,  310,  247, 1270, 3448, 1566,   15, 1198, 3687, 8794, 1972,  273,\n",
            "        8794, 1972,  273, 8794, 1972,  273, 8794, 1972,  273, 8794, 1972,  273,\n",
            "        8794, 1972,  273, 8794, 1972,  273, 8794, 1972], device='cuda:0')\n",
            "\n",
            "prompt: This is a\n",
            "generated_text: This is a great language model. For greater nuances of nuances of nuances of nuances of nuances of nuances of nuances of nuances\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc-autonumbering": true,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "toc-showtags": false,
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "223f3a0aac2c4967b3c6e2987d1bf870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a29eff3ec1424ca8887a0ef94a3c5f84",
              "IPY_MODEL_ae47417f22a44313a8a3e93c651d6a8a",
              "IPY_MODEL_23704bb22bce437e8daa47f4a9bf72be"
            ],
            "layout": "IPY_MODEL_b612c0f9a920493b8bdc646df1ae2f80"
          }
        },
        "a29eff3ec1424ca8887a0ef94a3c5f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d20c10ff506945d08bd8334d4fe3c1a5",
            "placeholder": "​",
            "style": "IPY_MODEL_cadd1d5ef7bd4998b9e00c64410141a2",
            "value": "Map:   0%"
          }
        },
        "ae47417f22a44313a8a3e93c651d6a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcd4ed79b9184062b51ce6e4be213982",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b9bfda7faf645e6a7b00ef99dc875eb",
            "value": 3
          }
        },
        "23704bb22bce437e8daa47f4a9bf72be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59a183ccacea40d78b2efbeccc156dbb",
            "placeholder": "​",
            "style": "IPY_MODEL_0f8d6072c6564226850d47d72aa99aef",
            "value": " 0/3 [00:00&lt;?, ? examples/s]"
          }
        },
        "b612c0f9a920493b8bdc646df1ae2f80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "d20c10ff506945d08bd8334d4fe3c1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cadd1d5ef7bd4998b9e00c64410141a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcd4ed79b9184062b51ce6e4be213982": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9bfda7faf645e6a7b00ef99dc875eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59a183ccacea40d78b2efbeccc156dbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f8d6072c6564226850d47d72aa99aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
