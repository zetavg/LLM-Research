resources:
  accelerators: A100:1
  use_spot: true

file_mounts:
  # Mount a presisted cloud storage.
  # See https://skypilot.readthedocs.io/en/latest/reference/storage.html for details.
  /llm_training_data:
    # Make sure this name is unique or you own this bucket. If it does not exists, SkyPilot will try to create a bucket with this name.
    name: llm-training-gcs-08
    store: gcs  # Could be either of [s3, gcs]
    mode: MOUNT
  /cache:
    name: zh-tw-pythia-skypilot-cache
    store: gcs
    mode: MOUNT

workdir: ./sky_workdir

setup: |
  sudo apt-get install git-lfs
  git lfs install --skip-repo
  git config --global credential.helper store
  conda create -q python=3.8 -n zh-tw-pythia -y
  conda activate zh-tw-pythia
  pip install -r requirements.txt
  # prepare cache
  rsync -a --exclude 'token' /cache/huggingface/. ~/.cache/huggingface/ --itemize-changes
  # nvtop
  wget https://github.com/Syllo/nvtop/releases/download/3.0.1/nvtop-3.0.1-x86_64.AppImage
  sudo chmod +x nvtop-3.0.1-x86_64.AppImage
  sudo cp nvtop-3.0.1-x86_64.AppImage /usr/local/bin/nvtop
  # locale
  sudo echo 'LANGUAGE=zh_TW.UTF-8' >> /etc/default/locale
  sudo echo 'LC_ALL=zh_TW.UTF-8' >> /etc/default/locale
  sudo echo 'zh_TW.UTF-8 UTF-' >> /etc/locale.gen
  sudo apt-get install locales -y
  sudo locale-gen

run: |
  conda activate zh-tw-pythia
  python train.py \
    --tokenizer='zetavg/test-pythia-zh-tw-tokenizer-50000-20230507' \
    --base_model='EleutherAI/pythia-1b' \
    --dataset='zetavg/zh-tw-wikipedia' \
    --dataset_column='markdown' \
    --dataset_split='train' \
    --cutoff_len=2048 \
    --train_data_limit=200000 \
    --per_device_train_batch_size=8 \
    --train_params='embed'  \
    --output_dir='/llm_training_data/zh_tw_pythia/test-skypilot-02' \
    --save_steps=1000 \
    --run_name='test-skypilot-02' \
    --wandb_project='test-project' \
    --push_to_hf --hf_hub_private_repo
  # save cache
  rsync -a --exclude 'token' ~/.cache/huggingface/. /cache/huggingface/ --itemize-changes
