{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMl0KgrHdDsDiRv4n6PfMYy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zetavg/LLM-Research/blob/main/Wikipedia_Random_Page_Summaries_Dataset_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wikipedia Random Page Summaries Dataset Generator\n",
        "\n",
        "Collect random Wikipedia page summaries and save them into a dataset for further training. Optionally uploads the dataset to Hugging Face Hub."
      ],
      "metadata": {
        "id": "TUnPlvVhb7BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Settings { display-mode: \"form\", run: \"auto\" }\n",
        "\n",
        "wikipedia_lang = \"zh-tw\"  # @param {type:\"string\"}\n",
        "number_of_pages = 5000  # @param {type:\"integer\"}\n",
        "\n",
        "hf_dataset_set_as_private = False  # @param {type:\"boolean\"}\n",
        "# @markdown If set to blank, the dataset will not be uploaded to Hugging Face. **Note that this dataset may be overwritten if exists**:\n",
        "hf_dataset_name = \"zetavg/wikipedia_random_page_summaries_zh_tw_5k\"  # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "KhpkPZcpcbD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets huggingface_hub gradio"
      ],
      "metadata": {
        "id": "NwBpSStQdcIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hugging Face Login { display-mode: \"form\" }\n",
        "# @markdown Make sure to have this done before running the \"Upload the dataset to Hugging Face Hub\" section.\n",
        "\n",
        "from huggingface_hub import HfFolder, whoami, notebook_login\n",
        "\n",
        "upload_to_hf = False\n",
        "if hf_dataset_name:\n",
        "    upload_to_hf = True\n",
        "\n",
        "\n",
        "def check_login_status():\n",
        "    folder = HfFolder()\n",
        "    token = folder.get_token()\n",
        "\n",
        "    if token is not None:\n",
        "        try:\n",
        "            user_info = whoami(token)\n",
        "            username = user_info[\"name\"]\n",
        "            return username\n",
        "        except Exception:\n",
        "            return None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "if upload_to_hf:\n",
        "    username = None\n",
        "    username = check_login_status()\n",
        "    if not username:\n",
        "        notebook_login()\n",
        "    else:\n",
        "        print(f\"Already login as {username}\")"
      ],
      "metadata": {
        "id": "0rAjk7I8eHJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Install necessary packages.\n",
        "\n",
        "# @markdown A patched version of the wikipedia package to support zh-tw.\n",
        "!pip install git+https://github.com/zetavg/python_wikipedia\n",
        "\n",
        "# @markdown 「盤古之白」(https://github.com/vinta/pangu.js).\n",
        "!pip install pangu"
      ],
      "metadata": {
        "id": "3zxFf6vCgleW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the dataset"
      ],
      "metadata": {
        "id": "dq2Py-3pgzP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Test if we can get pages from wikipedia\n",
        "\n",
        "import wikipedia\n",
        "import pangu\n",
        "\n",
        "for _ in range(10):\n",
        "    try:\n",
        "        wikipedia.set_lang(wikipedia_lang)\n",
        "        page_title = wikipedia.random()\n",
        "        print(\"Page Title: \", page_title)\n",
        "        # @markdown Note: 中英文中間的空格（盤古之白 XD）在這邊扮演了重要角色，因為根據大多數 tokenizer 實作的原理，英文字前面沒空格的話會被轉成另一個不一樣的 token。\n",
        "        print(pangu.spacing_text(\n",
        "            wikipedia.summary(page_title)\n",
        "        ))\n",
        "        # @markdown Note: \"Page Title\" 有些可能是簡體中文，這是正常的。因為它們是維基百科未經轉換的原始頁面名稱。\n",
        "    except Exception as e:\n",
        "        # @markdown Note: 有些因為消歧義產生的 error 可以忽略。\n",
        "        print(\"Error: \", e)\n",
        "    print()"
      ],
      "metadata": {
        "id": "krGsDAKSg2Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Define how the data should be loaded. See: https://shareg.pt/kx4UbKd.\n",
        "\n",
        "import queue\n",
        "from wikipedia import DisambiguationError\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from threading import Semaphore\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def get_random_page_summary(sentences=0):\n",
        "    try:\n",
        "        page_title = wikipedia.random()\n",
        "        page_summary = wikipedia.summary(page_title)\n",
        "        page_summary = pangu.spacing_text(page_summary)\n",
        "        return {\n",
        "            'page_title': page_title,\n",
        "            'page_summary': page_summary,\n",
        "        }\n",
        "    except DisambiguationError:  # \"...\" may refer to \"...\" or \"...\"\n",
        "        # Just retry\n",
        "        return get_random_page_summary(sentences)\n",
        "    except Exception as e:  # Ignore other errors\n",
        "        print(e)\n",
        "        return None\n",
        "\n",
        "\n",
        "def fetch_page_and_update_queue(\n",
        "        sentences, fetched_pages_queue, fetch_semaphore):\n",
        "    fetch_semaphore.acquire()\n",
        "    result = get_random_page_summary(sentences)\n",
        "    fetched_pages_queue.put(result)\n",
        "\n",
        "\n",
        "def data_generator(\n",
        "        count=1000,\n",
        "        sentences=0,  # 0: Unlimited\n",
        "        max_workers=50,\n",
        "        max_buffer_size=500):\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        fetched_pages_queue = queue.Queue()\n",
        "        fetch_semaphore = Semaphore(max_buffer_size)\n",
        "\n",
        "        for _ in range(count):\n",
        "            executor.submit(fetch_page_and_update_queue,\n",
        "                            sentences,\n",
        "                            fetched_pages_queue, fetch_semaphore)\n",
        "\n",
        "        progress_bar = tqdm(total=count)\n",
        "        yielded_count = 0\n",
        "        errored_count = 0\n",
        "\n",
        "        for _ in range(count):\n",
        "            result = fetched_pages_queue.get()\n",
        "            fetch_semaphore.release()\n",
        "\n",
        "            if result:\n",
        "                yielded_count += 1\n",
        "                yield result\n",
        "            else:\n",
        "                errored_count += 1\n",
        "\n",
        "            fetched_count = yielded_count + errored_count + \\\n",
        "                len(fetched_pages_queue.queue)\n",
        "            description = f\"Pages processed/fetched: {yielded_count}/{fetched_count}\"\n",
        "            if errored_count > 0:\n",
        "                description = f\"Pages processed/errored/fetched: {yielded_count}/{errored_count}/{fetched_count}\"\n",
        "            progress_bar.set_description(description)\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        progress_bar.close()"
      ],
      "metadata": {
        "id": "T6Oo-zCuhX9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create the dataset\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "ds = Dataset.from_generator(\n",
        "    data_generator,\n",
        "    gen_kwargs={\n",
        "        'count': number_of_pages\n",
        "    })"
      ],
      "metadata": {
        "id": "xZBHYEGKdivE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preview the created dataset\n",
        "import json\n",
        "\n",
        "print(\"features: \", ds.features)\n",
        "print(\"num_rows: \", ds.num_rows)\n",
        "\n",
        "print(\"preview: \")\n",
        "for i in range(10):\n",
        "    item = ds[i]\n",
        "    print(json.dumps(item, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "uMwa5nAHkBSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the dataset to Hugging Face Hub"
      ],
      "metadata": {
        "id": "y7BU1DkElQNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if upload_to_hf:\n",
        "    print(f\"Uploading {'private' if hf_dataset_set_as_private else 'public'} dataset '{hf_dataset_name}' to Hugging Face...\")\n",
        "    ds.push_to_hub(\n",
        "        hf_dataset_name,\n",
        "        private=hf_dataset_set_as_private)\n",
        "    print(f\"Dataset uploaded: https://huggingface.co/datasets/{hf_dataset_name}.\")\n",
        "else:\n",
        "    print(\"Upload skipped.\")"
      ],
      "metadata": {
        "id": "OFc3Q1nkfHoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Preview the uploaded dataset\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "if upload_to_hf:\n",
        "    ds_from_hf = load_dataset(hf_dataset_name)['train']\n",
        "\n",
        "    print(\"features: \", ds_from_hf.features)\n",
        "    print(\"num_rows: \", ds_from_hf.num_rows)\n",
        "\n",
        "    print(\"preview: \")\n",
        "    for i in range(10):\n",
        "        item = ds_from_hf[i]\n",
        "        print(json.dumps(item, indent=2, ensure_ascii=False))"
      ],
      "metadata": {
        "id": "E6-w59t1lUwc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}